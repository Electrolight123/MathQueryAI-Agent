import { Callout } from 'nextra/components'
import Tabs from '@theme/Tabs'
import TabItem from '@theme/TabItem'

# ğŸ§® MathQueryAI-Agent

A smart AI-powered assistant built with **Streamlit**, **LangChain**, and **Google Gemma 2**, capable of solving complex text-based mathematical problems and searching Wikipedia for relevant information. This agent uses multiple tools â€” a calculator, logical reasoning chain, and a knowledge retriever â€” all orchestrated through a `zero-shot-react-description` LangChain agent.

---

## ğŸš€ Features

- ğŸ§  Converts real-world word problems into structured math problems and solves them step-by-step.
- ğŸŒ Wikipedia-powered research for context-aware knowledge support.
- ğŸ¤– Agent-based architecture with reasoning and math tools.
- ğŸ” Secure integration with Groq API for LLM access.
- âš¡ Interactive UI with Streamlit for real-time Q&A.

---

## ğŸ“ Folder & File Structure

```bash
.
â”œâ”€â”€ app.py                 # ğŸš€ Main entry point for Streamlit app
â”œâ”€â”€ requirements.txt       # ğŸ“¦ Python dependencies

```

---

## ğŸ“¦ Installation
<Tabs> <TabItem value="local" label="ğŸ”§ Local Setup" default>

# 1. Clone the repository
```bash
git clone https://github.com/your-username/MathQueryAI-Agent.git
cd MathQueryAI-Agent
```
# 2. Create a virtual environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```
# 3. Install dependencies
```bash
pip install -r requirements.txt
```

# 4. Run the app
```bash
streamlit run app.py
```

</TabItem> </Tabs>

---

## ğŸ”‘ Configuration
You must enter your **Groq API Key** in the sidebar when prompted to access the Gemma 2 LLM.

> ğŸ” **Groq API Key** is required for the LLM to function. The app wonâ€™t proceed without it.

---

## ğŸ§  How It Works
The app initializes an agent with three powerful tools:

## ğŸ“š Wikipedia Tool
Uses WikipediaAPIWrapper to fetch topic summaries.

## â• Calculator Tool
Uses LLMMathChain to solve numeric problems.

## ğŸ’¡ Reasoning Tool
Custom PromptTemplate and LLMChain to solve logic-based queries.

Once a user inputs a question, the agent selects the most appropriate tool (or combination) to generate a clear, step-by-step answer using Gemma 2 LLM hosted by Groq.

---

âœï¸ Usage Example
css
Copy
Edit
I have 5 bananas and 7 grapes. I eat 2 bananas and give away 3 grapes. 
Then I buy a dozen apples and 2 packs of blueberries. Each pack of blueberries contains 25 berries. 
How many total pieces of fruit do I have at the end?
âœ… The assistant parses the logic, performs calculations, and returns:

ğŸ§® Final Count: 67 fruits
ğŸ“ Step-by-step explanation (pointwise)

---

ğŸ‘¨â€ğŸ’» Technologies Used
Tool	Purpose
ğŸŸ£ Streamlit	Web App UI
ğŸ§  LangChain	Agent management & tool orchestration
ğŸŸ¡ Groq (Gemma 2)	LLM backend
ğŸ“š Wikipedia API	Knowledge retrieval
â— LLMMathChain	Math problem solving

---

ğŸ™Œ Contributing
Contributions are welcome! ğŸš€

Fork the repo

Create your feature branch (git checkout -b feature/AmazingFeature)

Commit your changes (git commit -m 'Add amazing feature')

Push to the branch (git push origin feature/AmazingFeature)

Open a pull request

---

ğŸªª License
This project is licensed under the MIT License â€” see the LICENSE file for details.

ğŸ’¡ Want to use your own LLM backend? Just replace the `ChatGroq` wrapper with another LangChain-compatible LLM in `app.py`. 
